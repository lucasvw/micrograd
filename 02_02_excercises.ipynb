{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasvanwalstijn/miniconda3/envs/fastai/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt') as file:\n",
    "    names = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = set()\n",
    "for name in names:\n",
    "    for letter in name:\n",
    "        letters.add(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = list(sorted(letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First idea is to do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "emma\n",
      ".e -> m\n",
      "em -> m\n",
      "mm -> a\n",
      "ma -> .\n",
      "########\n",
      "########\n",
      "olivia\n",
      ".o -> l\n",
      "ol -> i\n",
      "li -> v\n",
      "iv -> i\n",
      "vi -> a\n",
      "ia -> .\n",
      "########\n",
      "########\n",
      "ava\n",
      ".a -> v\n",
      "av -> a\n",
      "va -> .\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "for name in names[:3]:\n",
    "    print('########')\n",
    "    print(name)\n",
    "    name = '.' + name + '.'\n",
    "    for i,j,k in (zip(name, name[1:], name[2:])):\n",
    "        print(i+j, '->',k)\n",
    "    print('########')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do this, then we have the problem that the very first 3-gram is actually not starting from a neutral state, it starts with \".x\" where x is the first letter of the name. So instead, we need to do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "emma\n",
      ".. -> e\n",
      ".e -> m\n",
      "em -> m\n",
      "mm -> a\n",
      "ma -> .\n",
      "########\n",
      "########\n",
      "olivia\n",
      ".. -> o\n",
      ".o -> l\n",
      "ol -> i\n",
      "li -> v\n",
      "iv -> i\n",
      "vi -> a\n",
      "ia -> .\n",
      "########\n",
      "########\n",
      "ava\n",
      ".. -> a\n",
      ".a -> v\n",
      "av -> a\n",
      "va -> .\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "for name in names[:3]:\n",
    "    print('########')\n",
    "    print(name)\n",
    "    name = '..' + name + '.'\n",
    "    for i,j,k in (zip(name, name[1:], name[2:])):\n",
    "        print(i+j, '->',k)\n",
    "    print('########')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to append `..` to the beginning, then we start with a neutral state and predict the first letter from that neutral state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to construct the matrix in which we will store the counts. Previously we had a 27x27 matrix to store all this information, but now that we have a trigram, so the row dimension will become larger. Because the row dimension will have to cover all the `..`, `.x` and `xy` combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With itertools product we can make these kind of combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2), (3, 3)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "list(product([1,2,3], [1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..',\n",
       " ('.', 'a'),\n",
       " ('.', 'b'),\n",
       " ('.', 'c'),\n",
       " ('.', 'd'),\n",
       " ('.', 'e'),\n",
       " ('.', 'f'),\n",
       " ('.', 'g'),\n",
       " ('.', 'h'),\n",
       " ('.', 'i'),\n",
       " ('.', 'j'),\n",
       " ('.', 'k'),\n",
       " ('.', 'l'),\n",
       " ('.', 'm'),\n",
       " ('.', 'n'),\n",
       " ('.', 'o'),\n",
       " ('.', 'p'),\n",
       " ('.', 'q'),\n",
       " ('.', 'r'),\n",
       " ('.', 's'),\n",
       " ('.', 't'),\n",
       " ('.', 'u'),\n",
       " ('.', 'v'),\n",
       " ('.', 'w'),\n",
       " ('.', 'x'),\n",
       " ('.', 'y'),\n",
       " ('.', 'z'),\n",
       " ('a', 'a'),\n",
       " ('a', 'b'),\n",
       " ('a', 'c'),\n",
       " ('a', 'd'),\n",
       " ('a', 'e'),\n",
       " ('a', 'f'),\n",
       " ('a', 'g'),\n",
       " ('a', 'h'),\n",
       " ('a', 'i'),\n",
       " ('a', 'j'),\n",
       " ('a', 'k'),\n",
       " ('a', 'l'),\n",
       " ('a', 'm'),\n",
       " ('a', 'n'),\n",
       " ('a', 'o'),\n",
       " ('a', 'p'),\n",
       " ('a', 'q'),\n",
       " ('a', 'r'),\n",
       " ('a', 's'),\n",
       " ('a', 't'),\n",
       " ('a', 'u'),\n",
       " ('a', 'v'),\n",
       " ('a', 'w'),\n",
       " ('a', 'x'),\n",
       " ('a', 'y'),\n",
       " ('a', 'z'),\n",
       " ('b', 'a'),\n",
       " ('b', 'b'),\n",
       " ('b', 'c'),\n",
       " ('b', 'd'),\n",
       " ('b', 'e'),\n",
       " ('b', 'f'),\n",
       " ('b', 'g'),\n",
       " ('b', 'h'),\n",
       " ('b', 'i'),\n",
       " ('b', 'j'),\n",
       " ('b', 'k'),\n",
       " ('b', 'l'),\n",
       " ('b', 'm'),\n",
       " ('b', 'n'),\n",
       " ('b', 'o'),\n",
       " ('b', 'p'),\n",
       " ('b', 'q'),\n",
       " ('b', 'r'),\n",
       " ('b', 's'),\n",
       " ('b', 't'),\n",
       " ('b', 'u'),\n",
       " ('b', 'v'),\n",
       " ('b', 'w'),\n",
       " ('b', 'x'),\n",
       " ('b', 'y'),\n",
       " ('b', 'z'),\n",
       " ('c', 'a'),\n",
       " ('c', 'b'),\n",
       " ('c', 'c'),\n",
       " ('c', 'd'),\n",
       " ('c', 'e'),\n",
       " ('c', 'f'),\n",
       " ('c', 'g'),\n",
       " ('c', 'h'),\n",
       " ('c', 'i'),\n",
       " ('c', 'j'),\n",
       " ('c', 'k'),\n",
       " ('c', 'l'),\n",
       " ('c', 'm'),\n",
       " ('c', 'n'),\n",
       " ('c', 'o'),\n",
       " ('c', 'p'),\n",
       " ('c', 'q'),\n",
       " ('c', 'r'),\n",
       " ('c', 's'),\n",
       " ('c', 't'),\n",
       " ('c', 'u'),\n",
       " ('c', 'v'),\n",
       " ('c', 'w'),\n",
       " ('c', 'x'),\n",
       " ('c', 'y'),\n",
       " ('c', 'z'),\n",
       " ('d', 'a'),\n",
       " ('d', 'b'),\n",
       " ('d', 'c'),\n",
       " ('d', 'd'),\n",
       " ('d', 'e'),\n",
       " ('d', 'f'),\n",
       " ('d', 'g'),\n",
       " ('d', 'h'),\n",
       " ('d', 'i'),\n",
       " ('d', 'j'),\n",
       " ('d', 'k'),\n",
       " ('d', 'l'),\n",
       " ('d', 'm'),\n",
       " ('d', 'n'),\n",
       " ('d', 'o'),\n",
       " ('d', 'p'),\n",
       " ('d', 'q'),\n",
       " ('d', 'r'),\n",
       " ('d', 's'),\n",
       " ('d', 't'),\n",
       " ('d', 'u'),\n",
       " ('d', 'v'),\n",
       " ('d', 'w'),\n",
       " ('d', 'x'),\n",
       " ('d', 'y'),\n",
       " ('d', 'z'),\n",
       " ('e', 'a'),\n",
       " ('e', 'b'),\n",
       " ('e', 'c'),\n",
       " ('e', 'd'),\n",
       " ('e', 'e'),\n",
       " ('e', 'f'),\n",
       " ('e', 'g'),\n",
       " ('e', 'h'),\n",
       " ('e', 'i'),\n",
       " ('e', 'j'),\n",
       " ('e', 'k'),\n",
       " ('e', 'l'),\n",
       " ('e', 'm'),\n",
       " ('e', 'n'),\n",
       " ('e', 'o'),\n",
       " ('e', 'p'),\n",
       " ('e', 'q'),\n",
       " ('e', 'r'),\n",
       " ('e', 's'),\n",
       " ('e', 't'),\n",
       " ('e', 'u'),\n",
       " ('e', 'v'),\n",
       " ('e', 'w'),\n",
       " ('e', 'x'),\n",
       " ('e', 'y'),\n",
       " ('e', 'z'),\n",
       " ('f', 'a'),\n",
       " ('f', 'b'),\n",
       " ('f', 'c'),\n",
       " ('f', 'd'),\n",
       " ('f', 'e'),\n",
       " ('f', 'f'),\n",
       " ('f', 'g'),\n",
       " ('f', 'h'),\n",
       " ('f', 'i'),\n",
       " ('f', 'j'),\n",
       " ('f', 'k'),\n",
       " ('f', 'l'),\n",
       " ('f', 'm'),\n",
       " ('f', 'n'),\n",
       " ('f', 'o'),\n",
       " ('f', 'p'),\n",
       " ('f', 'q'),\n",
       " ('f', 'r'),\n",
       " ('f', 's'),\n",
       " ('f', 't'),\n",
       " ('f', 'u'),\n",
       " ('f', 'v'),\n",
       " ('f', 'w'),\n",
       " ('f', 'x'),\n",
       " ('f', 'y'),\n",
       " ('f', 'z'),\n",
       " ('g', 'a'),\n",
       " ('g', 'b'),\n",
       " ('g', 'c'),\n",
       " ('g', 'd'),\n",
       " ('g', 'e'),\n",
       " ('g', 'f'),\n",
       " ('g', 'g'),\n",
       " ('g', 'h'),\n",
       " ('g', 'i'),\n",
       " ('g', 'j'),\n",
       " ('g', 'k'),\n",
       " ('g', 'l'),\n",
       " ('g', 'm'),\n",
       " ('g', 'n'),\n",
       " ('g', 'o'),\n",
       " ('g', 'p'),\n",
       " ('g', 'q'),\n",
       " ('g', 'r'),\n",
       " ('g', 's'),\n",
       " ('g', 't'),\n",
       " ('g', 'u'),\n",
       " ('g', 'v'),\n",
       " ('g', 'w'),\n",
       " ('g', 'x'),\n",
       " ('g', 'y'),\n",
       " ('g', 'z'),\n",
       " ('h', 'a'),\n",
       " ('h', 'b'),\n",
       " ('h', 'c'),\n",
       " ('h', 'd'),\n",
       " ('h', 'e'),\n",
       " ('h', 'f'),\n",
       " ('h', 'g'),\n",
       " ('h', 'h'),\n",
       " ('h', 'i'),\n",
       " ('h', 'j'),\n",
       " ('h', 'k'),\n",
       " ('h', 'l'),\n",
       " ('h', 'm'),\n",
       " ('h', 'n'),\n",
       " ('h', 'o'),\n",
       " ('h', 'p'),\n",
       " ('h', 'q'),\n",
       " ('h', 'r'),\n",
       " ('h', 's'),\n",
       " ('h', 't'),\n",
       " ('h', 'u'),\n",
       " ('h', 'v'),\n",
       " ('h', 'w'),\n",
       " ('h', 'x'),\n",
       " ('h', 'y'),\n",
       " ('h', 'z'),\n",
       " ('i', 'a'),\n",
       " ('i', 'b'),\n",
       " ('i', 'c'),\n",
       " ('i', 'd'),\n",
       " ('i', 'e'),\n",
       " ('i', 'f'),\n",
       " ('i', 'g'),\n",
       " ('i', 'h'),\n",
       " ('i', 'i'),\n",
       " ('i', 'j'),\n",
       " ('i', 'k'),\n",
       " ('i', 'l'),\n",
       " ('i', 'm'),\n",
       " ('i', 'n'),\n",
       " ('i', 'o'),\n",
       " ('i', 'p'),\n",
       " ('i', 'q'),\n",
       " ('i', 'r'),\n",
       " ('i', 's'),\n",
       " ('i', 't'),\n",
       " ('i', 'u'),\n",
       " ('i', 'v'),\n",
       " ('i', 'w'),\n",
       " ('i', 'x'),\n",
       " ('i', 'y'),\n",
       " ('i', 'z'),\n",
       " ('j', 'a'),\n",
       " ('j', 'b'),\n",
       " ('j', 'c'),\n",
       " ('j', 'd'),\n",
       " ('j', 'e'),\n",
       " ('j', 'f'),\n",
       " ('j', 'g'),\n",
       " ('j', 'h'),\n",
       " ('j', 'i'),\n",
       " ('j', 'j'),\n",
       " ('j', 'k'),\n",
       " ('j', 'l'),\n",
       " ('j', 'm'),\n",
       " ('j', 'n'),\n",
       " ('j', 'o'),\n",
       " ('j', 'p'),\n",
       " ('j', 'q'),\n",
       " ('j', 'r'),\n",
       " ('j', 's'),\n",
       " ('j', 't'),\n",
       " ('j', 'u'),\n",
       " ('j', 'v'),\n",
       " ('j', 'w'),\n",
       " ('j', 'x'),\n",
       " ('j', 'y'),\n",
       " ('j', 'z'),\n",
       " ('k', 'a'),\n",
       " ('k', 'b'),\n",
       " ('k', 'c'),\n",
       " ('k', 'd'),\n",
       " ('k', 'e'),\n",
       " ('k', 'f'),\n",
       " ('k', 'g'),\n",
       " ('k', 'h'),\n",
       " ('k', 'i'),\n",
       " ('k', 'j'),\n",
       " ('k', 'k'),\n",
       " ('k', 'l'),\n",
       " ('k', 'm'),\n",
       " ('k', 'n'),\n",
       " ('k', 'o'),\n",
       " ('k', 'p'),\n",
       " ('k', 'q'),\n",
       " ('k', 'r'),\n",
       " ('k', 's'),\n",
       " ('k', 't'),\n",
       " ('k', 'u'),\n",
       " ('k', 'v'),\n",
       " ('k', 'w'),\n",
       " ('k', 'x'),\n",
       " ('k', 'y'),\n",
       " ('k', 'z'),\n",
       " ('l', 'a'),\n",
       " ('l', 'b'),\n",
       " ('l', 'c'),\n",
       " ('l', 'd'),\n",
       " ('l', 'e'),\n",
       " ('l', 'f'),\n",
       " ('l', 'g'),\n",
       " ('l', 'h'),\n",
       " ('l', 'i'),\n",
       " ('l', 'j'),\n",
       " ('l', 'k'),\n",
       " ('l', 'l'),\n",
       " ('l', 'm'),\n",
       " ('l', 'n'),\n",
       " ('l', 'o'),\n",
       " ('l', 'p'),\n",
       " ('l', 'q'),\n",
       " ('l', 'r'),\n",
       " ('l', 's'),\n",
       " ('l', 't'),\n",
       " ('l', 'u'),\n",
       " ('l', 'v'),\n",
       " ('l', 'w'),\n",
       " ('l', 'x'),\n",
       " ('l', 'y'),\n",
       " ('l', 'z'),\n",
       " ('m', 'a'),\n",
       " ('m', 'b'),\n",
       " ('m', 'c'),\n",
       " ('m', 'd'),\n",
       " ('m', 'e'),\n",
       " ('m', 'f'),\n",
       " ('m', 'g'),\n",
       " ('m', 'h'),\n",
       " ('m', 'i'),\n",
       " ('m', 'j'),\n",
       " ('m', 'k'),\n",
       " ('m', 'l'),\n",
       " ('m', 'm'),\n",
       " ('m', 'n'),\n",
       " ('m', 'o'),\n",
       " ('m', 'p'),\n",
       " ('m', 'q'),\n",
       " ('m', 'r'),\n",
       " ('m', 's'),\n",
       " ('m', 't'),\n",
       " ('m', 'u'),\n",
       " ('m', 'v'),\n",
       " ('m', 'w'),\n",
       " ('m', 'x'),\n",
       " ('m', 'y'),\n",
       " ('m', 'z'),\n",
       " ('n', 'a'),\n",
       " ('n', 'b'),\n",
       " ('n', 'c'),\n",
       " ('n', 'd'),\n",
       " ('n', 'e'),\n",
       " ('n', 'f'),\n",
       " ('n', 'g'),\n",
       " ('n', 'h'),\n",
       " ('n', 'i'),\n",
       " ('n', 'j'),\n",
       " ('n', 'k'),\n",
       " ('n', 'l'),\n",
       " ('n', 'm'),\n",
       " ('n', 'n'),\n",
       " ('n', 'o'),\n",
       " ('n', 'p'),\n",
       " ('n', 'q'),\n",
       " ('n', 'r'),\n",
       " ('n', 's'),\n",
       " ('n', 't'),\n",
       " ('n', 'u'),\n",
       " ('n', 'v'),\n",
       " ('n', 'w'),\n",
       " ('n', 'x'),\n",
       " ('n', 'y'),\n",
       " ('n', 'z'),\n",
       " ('o', 'a'),\n",
       " ('o', 'b'),\n",
       " ('o', 'c'),\n",
       " ('o', 'd'),\n",
       " ('o', 'e'),\n",
       " ('o', 'f'),\n",
       " ('o', 'g'),\n",
       " ('o', 'h'),\n",
       " ('o', 'i'),\n",
       " ('o', 'j'),\n",
       " ('o', 'k'),\n",
       " ('o', 'l'),\n",
       " ('o', 'm'),\n",
       " ('o', 'n'),\n",
       " ('o', 'o'),\n",
       " ('o', 'p'),\n",
       " ('o', 'q'),\n",
       " ('o', 'r'),\n",
       " ('o', 's'),\n",
       " ('o', 't'),\n",
       " ('o', 'u'),\n",
       " ('o', 'v'),\n",
       " ('o', 'w'),\n",
       " ('o', 'x'),\n",
       " ('o', 'y'),\n",
       " ('o', 'z'),\n",
       " ('p', 'a'),\n",
       " ('p', 'b'),\n",
       " ('p', 'c'),\n",
       " ('p', 'd'),\n",
       " ('p', 'e'),\n",
       " ('p', 'f'),\n",
       " ('p', 'g'),\n",
       " ('p', 'h'),\n",
       " ('p', 'i'),\n",
       " ('p', 'j'),\n",
       " ('p', 'k'),\n",
       " ('p', 'l'),\n",
       " ('p', 'm'),\n",
       " ('p', 'n'),\n",
       " ('p', 'o'),\n",
       " ('p', 'p'),\n",
       " ('p', 'q'),\n",
       " ('p', 'r'),\n",
       " ('p', 's'),\n",
       " ('p', 't'),\n",
       " ('p', 'u'),\n",
       " ('p', 'v'),\n",
       " ('p', 'w'),\n",
       " ('p', 'x'),\n",
       " ('p', 'y'),\n",
       " ('p', 'z'),\n",
       " ('q', 'a'),\n",
       " ('q', 'b'),\n",
       " ('q', 'c'),\n",
       " ('q', 'd'),\n",
       " ('q', 'e'),\n",
       " ('q', 'f'),\n",
       " ('q', 'g'),\n",
       " ('q', 'h'),\n",
       " ('q', 'i'),\n",
       " ('q', 'j'),\n",
       " ('q', 'k'),\n",
       " ('q', 'l'),\n",
       " ('q', 'm'),\n",
       " ('q', 'n'),\n",
       " ('q', 'o'),\n",
       " ('q', 'p'),\n",
       " ('q', 'q'),\n",
       " ('q', 'r'),\n",
       " ('q', 's'),\n",
       " ('q', 't'),\n",
       " ('q', 'u'),\n",
       " ('q', 'v'),\n",
       " ('q', 'w'),\n",
       " ('q', 'x'),\n",
       " ('q', 'y'),\n",
       " ('q', 'z'),\n",
       " ('r', 'a'),\n",
       " ('r', 'b'),\n",
       " ('r', 'c'),\n",
       " ('r', 'd'),\n",
       " ('r', 'e'),\n",
       " ('r', 'f'),\n",
       " ('r', 'g'),\n",
       " ('r', 'h'),\n",
       " ('r', 'i'),\n",
       " ('r', 'j'),\n",
       " ('r', 'k'),\n",
       " ('r', 'l'),\n",
       " ('r', 'm'),\n",
       " ('r', 'n'),\n",
       " ('r', 'o'),\n",
       " ('r', 'p'),\n",
       " ('r', 'q'),\n",
       " ('r', 'r'),\n",
       " ('r', 's'),\n",
       " ('r', 't'),\n",
       " ('r', 'u'),\n",
       " ('r', 'v'),\n",
       " ('r', 'w'),\n",
       " ('r', 'x'),\n",
       " ('r', 'y'),\n",
       " ('r', 'z'),\n",
       " ('s', 'a'),\n",
       " ('s', 'b'),\n",
       " ('s', 'c'),\n",
       " ('s', 'd'),\n",
       " ('s', 'e'),\n",
       " ('s', 'f'),\n",
       " ('s', 'g'),\n",
       " ('s', 'h'),\n",
       " ('s', 'i'),\n",
       " ('s', 'j'),\n",
       " ('s', 'k'),\n",
       " ('s', 'l'),\n",
       " ('s', 'm'),\n",
       " ('s', 'n'),\n",
       " ('s', 'o'),\n",
       " ('s', 'p'),\n",
       " ('s', 'q'),\n",
       " ('s', 'r'),\n",
       " ('s', 's'),\n",
       " ('s', 't'),\n",
       " ('s', 'u'),\n",
       " ('s', 'v'),\n",
       " ('s', 'w'),\n",
       " ('s', 'x'),\n",
       " ('s', 'y'),\n",
       " ('s', 'z'),\n",
       " ('t', 'a'),\n",
       " ('t', 'b'),\n",
       " ('t', 'c'),\n",
       " ('t', 'd'),\n",
       " ('t', 'e'),\n",
       " ('t', 'f'),\n",
       " ('t', 'g'),\n",
       " ('t', 'h'),\n",
       " ('t', 'i'),\n",
       " ('t', 'j'),\n",
       " ('t', 'k'),\n",
       " ('t', 'l'),\n",
       " ('t', 'm'),\n",
       " ('t', 'n'),\n",
       " ('t', 'o'),\n",
       " ('t', 'p'),\n",
       " ('t', 'q'),\n",
       " ('t', 'r'),\n",
       " ('t', 's'),\n",
       " ('t', 't'),\n",
       " ('t', 'u'),\n",
       " ('t', 'v'),\n",
       " ('t', 'w'),\n",
       " ('t', 'x'),\n",
       " ('t', 'y'),\n",
       " ('t', 'z'),\n",
       " ('u', 'a'),\n",
       " ('u', 'b'),\n",
       " ('u', 'c'),\n",
       " ('u', 'd'),\n",
       " ('u', 'e'),\n",
       " ('u', 'f'),\n",
       " ('u', 'g'),\n",
       " ('u', 'h'),\n",
       " ('u', 'i'),\n",
       " ('u', 'j'),\n",
       " ('u', 'k'),\n",
       " ('u', 'l'),\n",
       " ('u', 'm'),\n",
       " ('u', 'n'),\n",
       " ('u', 'o'),\n",
       " ('u', 'p'),\n",
       " ('u', 'q'),\n",
       " ('u', 'r'),\n",
       " ('u', 's'),\n",
       " ('u', 't'),\n",
       " ('u', 'u'),\n",
       " ('u', 'v'),\n",
       " ('u', 'w'),\n",
       " ('u', 'x'),\n",
       " ('u', 'y'),\n",
       " ('u', 'z'),\n",
       " ('v', 'a'),\n",
       " ('v', 'b'),\n",
       " ('v', 'c'),\n",
       " ('v', 'd'),\n",
       " ('v', 'e'),\n",
       " ('v', 'f'),\n",
       " ('v', 'g'),\n",
       " ('v', 'h'),\n",
       " ('v', 'i'),\n",
       " ('v', 'j'),\n",
       " ('v', 'k'),\n",
       " ('v', 'l'),\n",
       " ('v', 'm'),\n",
       " ('v', 'n'),\n",
       " ('v', 'o'),\n",
       " ('v', 'p'),\n",
       " ('v', 'q'),\n",
       " ('v', 'r'),\n",
       " ('v', 's'),\n",
       " ('v', 't'),\n",
       " ('v', 'u'),\n",
       " ('v', 'v'),\n",
       " ('v', 'w'),\n",
       " ('v', 'x'),\n",
       " ('v', 'y'),\n",
       " ('v', 'z'),\n",
       " ('w', 'a'),\n",
       " ('w', 'b'),\n",
       " ('w', 'c'),\n",
       " ('w', 'd'),\n",
       " ('w', 'e'),\n",
       " ('w', 'f'),\n",
       " ('w', 'g'),\n",
       " ('w', 'h'),\n",
       " ('w', 'i'),\n",
       " ('w', 'j'),\n",
       " ('w', 'k'),\n",
       " ('w', 'l'),\n",
       " ('w', 'm'),\n",
       " ('w', 'n'),\n",
       " ('w', 'o'),\n",
       " ('w', 'p'),\n",
       " ('w', 'q'),\n",
       " ('w', 'r'),\n",
       " ('w', 's'),\n",
       " ('w', 't'),\n",
       " ('w', 'u'),\n",
       " ('w', 'v'),\n",
       " ('w', 'w'),\n",
       " ('w', 'x'),\n",
       " ('w', 'y'),\n",
       " ('w', 'z'),\n",
       " ('x', 'a'),\n",
       " ('x', 'b'),\n",
       " ('x', 'c'),\n",
       " ('x', 'd'),\n",
       " ('x', 'e'),\n",
       " ('x', 'f'),\n",
       " ('x', 'g'),\n",
       " ('x', 'h'),\n",
       " ('x', 'i'),\n",
       " ('x', 'j'),\n",
       " ('x', 'k'),\n",
       " ('x', 'l'),\n",
       " ('x', 'm'),\n",
       " ('x', 'n'),\n",
       " ('x', 'o'),\n",
       " ('x', 'p'),\n",
       " ('x', 'q'),\n",
       " ('x', 'r'),\n",
       " ('x', 's'),\n",
       " ('x', 't'),\n",
       " ('x', 'u'),\n",
       " ('x', 'v'),\n",
       " ('x', 'w'),\n",
       " ('x', 'x'),\n",
       " ('x', 'y'),\n",
       " ('x', 'z'),\n",
       " ('y', 'a'),\n",
       " ('y', 'b'),\n",
       " ('y', 'c'),\n",
       " ('y', 'd'),\n",
       " ('y', 'e'),\n",
       " ('y', 'f'),\n",
       " ('y', 'g'),\n",
       " ('y', 'h'),\n",
       " ('y', 'i'),\n",
       " ('y', 'j'),\n",
       " ('y', 'k'),\n",
       " ('y', 'l'),\n",
       " ('y', 'm'),\n",
       " ('y', 'n'),\n",
       " ('y', 'o'),\n",
       " ('y', 'p'),\n",
       " ('y', 'q'),\n",
       " ('y', 'r'),\n",
       " ('y', 's'),\n",
       " ('y', 't'),\n",
       " ('y', 'u'),\n",
       " ('y', 'v'),\n",
       " ('y', 'w'),\n",
       " ('y', 'x'),\n",
       " ('y', 'y'),\n",
       " ('y', 'z'),\n",
       " ('z', 'a'),\n",
       " ('z', 'b'),\n",
       " ('z', 'c'),\n",
       " ('z', 'd'),\n",
       " ('z', 'e'),\n",
       " ('z', 'f'),\n",
       " ('z', 'g'),\n",
       " ('z', 'h'),\n",
       " ('z', 'i'),\n",
       " ('z', 'j'),\n",
       " ('z', 'k'),\n",
       " ('z', 'l'),\n",
       " ('z', 'm'),\n",
       " ('z', 'n'),\n",
       " ('z', 'o'),\n",
       " ('z', 'p'),\n",
       " ('z', 'q'),\n",
       " ('z', 'r'),\n",
       " ('z', 's'),\n",
       " ('z', 't'),\n",
       " ('z', 'u'),\n",
       " ('z', 'v'),\n",
       " ('z', 'w'),\n",
       " ('z', 'x'),\n",
       " ('z', 'y'),\n",
       " ('z', 'z')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinations = ['..'] + list(product('.', letters)) + list(product(letters, letters))\n",
    "combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_to_i = {c[0]+c[1]:i for i, c in enumerate(combinations)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'..': 0,\n",
       " '.a': 1,\n",
       " '.b': 2,\n",
       " '.c': 3,\n",
       " '.d': 4,\n",
       " '.e': 5,\n",
       " '.f': 6,\n",
       " '.g': 7,\n",
       " '.h': 8,\n",
       " '.i': 9,\n",
       " '.j': 10,\n",
       " '.k': 11,\n",
       " '.l': 12,\n",
       " '.m': 13,\n",
       " '.n': 14,\n",
       " '.o': 15,\n",
       " '.p': 16,\n",
       " '.q': 17,\n",
       " '.r': 18,\n",
       " '.s': 19,\n",
       " '.t': 20,\n",
       " '.u': 21,\n",
       " '.v': 22,\n",
       " '.w': 23,\n",
       " '.x': 24,\n",
       " '.y': 25,\n",
       " '.z': 26,\n",
       " 'aa': 27,\n",
       " 'ab': 28,\n",
       " 'ac': 29,\n",
       " 'ad': 30,\n",
       " 'ae': 31,\n",
       " 'af': 32,\n",
       " 'ag': 33,\n",
       " 'ah': 34,\n",
       " 'ai': 35,\n",
       " 'aj': 36,\n",
       " 'ak': 37,\n",
       " 'al': 38,\n",
       " 'am': 39,\n",
       " 'an': 40,\n",
       " 'ao': 41,\n",
       " 'ap': 42,\n",
       " 'aq': 43,\n",
       " 'ar': 44,\n",
       " 'as': 45,\n",
       " 'at': 46,\n",
       " 'au': 47,\n",
       " 'av': 48,\n",
       " 'aw': 49,\n",
       " 'ax': 50,\n",
       " 'ay': 51,\n",
       " 'az': 52,\n",
       " 'ba': 53,\n",
       " 'bb': 54,\n",
       " 'bc': 55,\n",
       " 'bd': 56,\n",
       " 'be': 57,\n",
       " 'bf': 58,\n",
       " 'bg': 59,\n",
       " 'bh': 60,\n",
       " 'bi': 61,\n",
       " 'bj': 62,\n",
       " 'bk': 63,\n",
       " 'bl': 64,\n",
       " 'bm': 65,\n",
       " 'bn': 66,\n",
       " 'bo': 67,\n",
       " 'bp': 68,\n",
       " 'bq': 69,\n",
       " 'br': 70,\n",
       " 'bs': 71,\n",
       " 'bt': 72,\n",
       " 'bu': 73,\n",
       " 'bv': 74,\n",
       " 'bw': 75,\n",
       " 'bx': 76,\n",
       " 'by': 77,\n",
       " 'bz': 78,\n",
       " 'ca': 79,\n",
       " 'cb': 80,\n",
       " 'cc': 81,\n",
       " 'cd': 82,\n",
       " 'ce': 83,\n",
       " 'cf': 84,\n",
       " 'cg': 85,\n",
       " 'ch': 86,\n",
       " 'ci': 87,\n",
       " 'cj': 88,\n",
       " 'ck': 89,\n",
       " 'cl': 90,\n",
       " 'cm': 91,\n",
       " 'cn': 92,\n",
       " 'co': 93,\n",
       " 'cp': 94,\n",
       " 'cq': 95,\n",
       " 'cr': 96,\n",
       " 'cs': 97,\n",
       " 'ct': 98,\n",
       " 'cu': 99,\n",
       " 'cv': 100,\n",
       " 'cw': 101,\n",
       " 'cx': 102,\n",
       " 'cy': 103,\n",
       " 'cz': 104,\n",
       " 'da': 105,\n",
       " 'db': 106,\n",
       " 'dc': 107,\n",
       " 'dd': 108,\n",
       " 'de': 109,\n",
       " 'df': 110,\n",
       " 'dg': 111,\n",
       " 'dh': 112,\n",
       " 'di': 113,\n",
       " 'dj': 114,\n",
       " 'dk': 115,\n",
       " 'dl': 116,\n",
       " 'dm': 117,\n",
       " 'dn': 118,\n",
       " 'do': 119,\n",
       " 'dp': 120,\n",
       " 'dq': 121,\n",
       " 'dr': 122,\n",
       " 'ds': 123,\n",
       " 'dt': 124,\n",
       " 'du': 125,\n",
       " 'dv': 126,\n",
       " 'dw': 127,\n",
       " 'dx': 128,\n",
       " 'dy': 129,\n",
       " 'dz': 130,\n",
       " 'ea': 131,\n",
       " 'eb': 132,\n",
       " 'ec': 133,\n",
       " 'ed': 134,\n",
       " 'ee': 135,\n",
       " 'ef': 136,\n",
       " 'eg': 137,\n",
       " 'eh': 138,\n",
       " 'ei': 139,\n",
       " 'ej': 140,\n",
       " 'ek': 141,\n",
       " 'el': 142,\n",
       " 'em': 143,\n",
       " 'en': 144,\n",
       " 'eo': 145,\n",
       " 'ep': 146,\n",
       " 'eq': 147,\n",
       " 'er': 148,\n",
       " 'es': 149,\n",
       " 'et': 150,\n",
       " 'eu': 151,\n",
       " 'ev': 152,\n",
       " 'ew': 153,\n",
       " 'ex': 154,\n",
       " 'ey': 155,\n",
       " 'ez': 156,\n",
       " 'fa': 157,\n",
       " 'fb': 158,\n",
       " 'fc': 159,\n",
       " 'fd': 160,\n",
       " 'fe': 161,\n",
       " 'ff': 162,\n",
       " 'fg': 163,\n",
       " 'fh': 164,\n",
       " 'fi': 165,\n",
       " 'fj': 166,\n",
       " 'fk': 167,\n",
       " 'fl': 168,\n",
       " 'fm': 169,\n",
       " 'fn': 170,\n",
       " 'fo': 171,\n",
       " 'fp': 172,\n",
       " 'fq': 173,\n",
       " 'fr': 174,\n",
       " 'fs': 175,\n",
       " 'ft': 176,\n",
       " 'fu': 177,\n",
       " 'fv': 178,\n",
       " 'fw': 179,\n",
       " 'fx': 180,\n",
       " 'fy': 181,\n",
       " 'fz': 182,\n",
       " 'ga': 183,\n",
       " 'gb': 184,\n",
       " 'gc': 185,\n",
       " 'gd': 186,\n",
       " 'ge': 187,\n",
       " 'gf': 188,\n",
       " 'gg': 189,\n",
       " 'gh': 190,\n",
       " 'gi': 191,\n",
       " 'gj': 192,\n",
       " 'gk': 193,\n",
       " 'gl': 194,\n",
       " 'gm': 195,\n",
       " 'gn': 196,\n",
       " 'go': 197,\n",
       " 'gp': 198,\n",
       " 'gq': 199,\n",
       " 'gr': 200,\n",
       " 'gs': 201,\n",
       " 'gt': 202,\n",
       " 'gu': 203,\n",
       " 'gv': 204,\n",
       " 'gw': 205,\n",
       " 'gx': 206,\n",
       " 'gy': 207,\n",
       " 'gz': 208,\n",
       " 'ha': 209,\n",
       " 'hb': 210,\n",
       " 'hc': 211,\n",
       " 'hd': 212,\n",
       " 'he': 213,\n",
       " 'hf': 214,\n",
       " 'hg': 215,\n",
       " 'hh': 216,\n",
       " 'hi': 217,\n",
       " 'hj': 218,\n",
       " 'hk': 219,\n",
       " 'hl': 220,\n",
       " 'hm': 221,\n",
       " 'hn': 222,\n",
       " 'ho': 223,\n",
       " 'hp': 224,\n",
       " 'hq': 225,\n",
       " 'hr': 226,\n",
       " 'hs': 227,\n",
       " 'ht': 228,\n",
       " 'hu': 229,\n",
       " 'hv': 230,\n",
       " 'hw': 231,\n",
       " 'hx': 232,\n",
       " 'hy': 233,\n",
       " 'hz': 234,\n",
       " 'ia': 235,\n",
       " 'ib': 236,\n",
       " 'ic': 237,\n",
       " 'id': 238,\n",
       " 'ie': 239,\n",
       " 'if': 240,\n",
       " 'ig': 241,\n",
       " 'ih': 242,\n",
       " 'ii': 243,\n",
       " 'ij': 244,\n",
       " 'ik': 245,\n",
       " 'il': 246,\n",
       " 'im': 247,\n",
       " 'in': 248,\n",
       " 'io': 249,\n",
       " 'ip': 250,\n",
       " 'iq': 251,\n",
       " 'ir': 252,\n",
       " 'is': 253,\n",
       " 'it': 254,\n",
       " 'iu': 255,\n",
       " 'iv': 256,\n",
       " 'iw': 257,\n",
       " 'ix': 258,\n",
       " 'iy': 259,\n",
       " 'iz': 260,\n",
       " 'ja': 261,\n",
       " 'jb': 262,\n",
       " 'jc': 263,\n",
       " 'jd': 264,\n",
       " 'je': 265,\n",
       " 'jf': 266,\n",
       " 'jg': 267,\n",
       " 'jh': 268,\n",
       " 'ji': 269,\n",
       " 'jj': 270,\n",
       " 'jk': 271,\n",
       " 'jl': 272,\n",
       " 'jm': 273,\n",
       " 'jn': 274,\n",
       " 'jo': 275,\n",
       " 'jp': 276,\n",
       " 'jq': 277,\n",
       " 'jr': 278,\n",
       " 'js': 279,\n",
       " 'jt': 280,\n",
       " 'ju': 281,\n",
       " 'jv': 282,\n",
       " 'jw': 283,\n",
       " 'jx': 284,\n",
       " 'jy': 285,\n",
       " 'jz': 286,\n",
       " 'ka': 287,\n",
       " 'kb': 288,\n",
       " 'kc': 289,\n",
       " 'kd': 290,\n",
       " 'ke': 291,\n",
       " 'kf': 292,\n",
       " 'kg': 293,\n",
       " 'kh': 294,\n",
       " 'ki': 295,\n",
       " 'kj': 296,\n",
       " 'kk': 297,\n",
       " 'kl': 298,\n",
       " 'km': 299,\n",
       " 'kn': 300,\n",
       " 'ko': 301,\n",
       " 'kp': 302,\n",
       " 'kq': 303,\n",
       " 'kr': 304,\n",
       " 'ks': 305,\n",
       " 'kt': 306,\n",
       " 'ku': 307,\n",
       " 'kv': 308,\n",
       " 'kw': 309,\n",
       " 'kx': 310,\n",
       " 'ky': 311,\n",
       " 'kz': 312,\n",
       " 'la': 313,\n",
       " 'lb': 314,\n",
       " 'lc': 315,\n",
       " 'ld': 316,\n",
       " 'le': 317,\n",
       " 'lf': 318,\n",
       " 'lg': 319,\n",
       " 'lh': 320,\n",
       " 'li': 321,\n",
       " 'lj': 322,\n",
       " 'lk': 323,\n",
       " 'll': 324,\n",
       " 'lm': 325,\n",
       " 'ln': 326,\n",
       " 'lo': 327,\n",
       " 'lp': 328,\n",
       " 'lq': 329,\n",
       " 'lr': 330,\n",
       " 'ls': 331,\n",
       " 'lt': 332,\n",
       " 'lu': 333,\n",
       " 'lv': 334,\n",
       " 'lw': 335,\n",
       " 'lx': 336,\n",
       " 'ly': 337,\n",
       " 'lz': 338,\n",
       " 'ma': 339,\n",
       " 'mb': 340,\n",
       " 'mc': 341,\n",
       " 'md': 342,\n",
       " 'me': 343,\n",
       " 'mf': 344,\n",
       " 'mg': 345,\n",
       " 'mh': 346,\n",
       " 'mi': 347,\n",
       " 'mj': 348,\n",
       " 'mk': 349,\n",
       " 'ml': 350,\n",
       " 'mm': 351,\n",
       " 'mn': 352,\n",
       " 'mo': 353,\n",
       " 'mp': 354,\n",
       " 'mq': 355,\n",
       " 'mr': 356,\n",
       " 'ms': 357,\n",
       " 'mt': 358,\n",
       " 'mu': 359,\n",
       " 'mv': 360,\n",
       " 'mw': 361,\n",
       " 'mx': 362,\n",
       " 'my': 363,\n",
       " 'mz': 364,\n",
       " 'na': 365,\n",
       " 'nb': 366,\n",
       " 'nc': 367,\n",
       " 'nd': 368,\n",
       " 'ne': 369,\n",
       " 'nf': 370,\n",
       " 'ng': 371,\n",
       " 'nh': 372,\n",
       " 'ni': 373,\n",
       " 'nj': 374,\n",
       " 'nk': 375,\n",
       " 'nl': 376,\n",
       " 'nm': 377,\n",
       " 'nn': 378,\n",
       " 'no': 379,\n",
       " 'np': 380,\n",
       " 'nq': 381,\n",
       " 'nr': 382,\n",
       " 'ns': 383,\n",
       " 'nt': 384,\n",
       " 'nu': 385,\n",
       " 'nv': 386,\n",
       " 'nw': 387,\n",
       " 'nx': 388,\n",
       " 'ny': 389,\n",
       " 'nz': 390,\n",
       " 'oa': 391,\n",
       " 'ob': 392,\n",
       " 'oc': 393,\n",
       " 'od': 394,\n",
       " 'oe': 395,\n",
       " 'of': 396,\n",
       " 'og': 397,\n",
       " 'oh': 398,\n",
       " 'oi': 399,\n",
       " 'oj': 400,\n",
       " 'ok': 401,\n",
       " 'ol': 402,\n",
       " 'om': 403,\n",
       " 'on': 404,\n",
       " 'oo': 405,\n",
       " 'op': 406,\n",
       " 'oq': 407,\n",
       " 'or': 408,\n",
       " 'os': 409,\n",
       " 'ot': 410,\n",
       " 'ou': 411,\n",
       " 'ov': 412,\n",
       " 'ow': 413,\n",
       " 'ox': 414,\n",
       " 'oy': 415,\n",
       " 'oz': 416,\n",
       " 'pa': 417,\n",
       " 'pb': 418,\n",
       " 'pc': 419,\n",
       " 'pd': 420,\n",
       " 'pe': 421,\n",
       " 'pf': 422,\n",
       " 'pg': 423,\n",
       " 'ph': 424,\n",
       " 'pi': 425,\n",
       " 'pj': 426,\n",
       " 'pk': 427,\n",
       " 'pl': 428,\n",
       " 'pm': 429,\n",
       " 'pn': 430,\n",
       " 'po': 431,\n",
       " 'pp': 432,\n",
       " 'pq': 433,\n",
       " 'pr': 434,\n",
       " 'ps': 435,\n",
       " 'pt': 436,\n",
       " 'pu': 437,\n",
       " 'pv': 438,\n",
       " 'pw': 439,\n",
       " 'px': 440,\n",
       " 'py': 441,\n",
       " 'pz': 442,\n",
       " 'qa': 443,\n",
       " 'qb': 444,\n",
       " 'qc': 445,\n",
       " 'qd': 446,\n",
       " 'qe': 447,\n",
       " 'qf': 448,\n",
       " 'qg': 449,\n",
       " 'qh': 450,\n",
       " 'qi': 451,\n",
       " 'qj': 452,\n",
       " 'qk': 453,\n",
       " 'ql': 454,\n",
       " 'qm': 455,\n",
       " 'qn': 456,\n",
       " 'qo': 457,\n",
       " 'qp': 458,\n",
       " 'qq': 459,\n",
       " 'qr': 460,\n",
       " 'qs': 461,\n",
       " 'qt': 462,\n",
       " 'qu': 463,\n",
       " 'qv': 464,\n",
       " 'qw': 465,\n",
       " 'qx': 466,\n",
       " 'qy': 467,\n",
       " 'qz': 468,\n",
       " 'ra': 469,\n",
       " 'rb': 470,\n",
       " 'rc': 471,\n",
       " 'rd': 472,\n",
       " 're': 473,\n",
       " 'rf': 474,\n",
       " 'rg': 475,\n",
       " 'rh': 476,\n",
       " 'ri': 477,\n",
       " 'rj': 478,\n",
       " 'rk': 479,\n",
       " 'rl': 480,\n",
       " 'rm': 481,\n",
       " 'rn': 482,\n",
       " 'ro': 483,\n",
       " 'rp': 484,\n",
       " 'rq': 485,\n",
       " 'rr': 486,\n",
       " 'rs': 487,\n",
       " 'rt': 488,\n",
       " 'ru': 489,\n",
       " 'rv': 490,\n",
       " 'rw': 491,\n",
       " 'rx': 492,\n",
       " 'ry': 493,\n",
       " 'rz': 494,\n",
       " 'sa': 495,\n",
       " 'sb': 496,\n",
       " 'sc': 497,\n",
       " 'sd': 498,\n",
       " 'se': 499,\n",
       " 'sf': 500,\n",
       " 'sg': 501,\n",
       " 'sh': 502,\n",
       " 'si': 503,\n",
       " 'sj': 504,\n",
       " 'sk': 505,\n",
       " 'sl': 506,\n",
       " 'sm': 507,\n",
       " 'sn': 508,\n",
       " 'so': 509,\n",
       " 'sp': 510,\n",
       " 'sq': 511,\n",
       " 'sr': 512,\n",
       " 'ss': 513,\n",
       " 'st': 514,\n",
       " 'su': 515,\n",
       " 'sv': 516,\n",
       " 'sw': 517,\n",
       " 'sx': 518,\n",
       " 'sy': 519,\n",
       " 'sz': 520,\n",
       " 'ta': 521,\n",
       " 'tb': 522,\n",
       " 'tc': 523,\n",
       " 'td': 524,\n",
       " 'te': 525,\n",
       " 'tf': 526,\n",
       " 'tg': 527,\n",
       " 'th': 528,\n",
       " 'ti': 529,\n",
       " 'tj': 530,\n",
       " 'tk': 531,\n",
       " 'tl': 532,\n",
       " 'tm': 533,\n",
       " 'tn': 534,\n",
       " 'to': 535,\n",
       " 'tp': 536,\n",
       " 'tq': 537,\n",
       " 'tr': 538,\n",
       " 'ts': 539,\n",
       " 'tt': 540,\n",
       " 'tu': 541,\n",
       " 'tv': 542,\n",
       " 'tw': 543,\n",
       " 'tx': 544,\n",
       " 'ty': 545,\n",
       " 'tz': 546,\n",
       " 'ua': 547,\n",
       " 'ub': 548,\n",
       " 'uc': 549,\n",
       " 'ud': 550,\n",
       " 'ue': 551,\n",
       " 'uf': 552,\n",
       " 'ug': 553,\n",
       " 'uh': 554,\n",
       " 'ui': 555,\n",
       " 'uj': 556,\n",
       " 'uk': 557,\n",
       " 'ul': 558,\n",
       " 'um': 559,\n",
       " 'un': 560,\n",
       " 'uo': 561,\n",
       " 'up': 562,\n",
       " 'uq': 563,\n",
       " 'ur': 564,\n",
       " 'us': 565,\n",
       " 'ut': 566,\n",
       " 'uu': 567,\n",
       " 'uv': 568,\n",
       " 'uw': 569,\n",
       " 'ux': 570,\n",
       " 'uy': 571,\n",
       " 'uz': 572,\n",
       " 'va': 573,\n",
       " 'vb': 574,\n",
       " 'vc': 575,\n",
       " 'vd': 576,\n",
       " 've': 577,\n",
       " 'vf': 578,\n",
       " 'vg': 579,\n",
       " 'vh': 580,\n",
       " 'vi': 581,\n",
       " 'vj': 582,\n",
       " 'vk': 583,\n",
       " 'vl': 584,\n",
       " 'vm': 585,\n",
       " 'vn': 586,\n",
       " 'vo': 587,\n",
       " 'vp': 588,\n",
       " 'vq': 589,\n",
       " 'vr': 590,\n",
       " 'vs': 591,\n",
       " 'vt': 592,\n",
       " 'vu': 593,\n",
       " 'vv': 594,\n",
       " 'vw': 595,\n",
       " 'vx': 596,\n",
       " 'vy': 597,\n",
       " 'vz': 598,\n",
       " 'wa': 599,\n",
       " 'wb': 600,\n",
       " 'wc': 601,\n",
       " 'wd': 602,\n",
       " 'we': 603,\n",
       " 'wf': 604,\n",
       " 'wg': 605,\n",
       " 'wh': 606,\n",
       " 'wi': 607,\n",
       " 'wj': 608,\n",
       " 'wk': 609,\n",
       " 'wl': 610,\n",
       " 'wm': 611,\n",
       " 'wn': 612,\n",
       " 'wo': 613,\n",
       " 'wp': 614,\n",
       " 'wq': 615,\n",
       " 'wr': 616,\n",
       " 'ws': 617,\n",
       " 'wt': 618,\n",
       " 'wu': 619,\n",
       " 'wv': 620,\n",
       " 'ww': 621,\n",
       " 'wx': 622,\n",
       " 'wy': 623,\n",
       " 'wz': 624,\n",
       " 'xa': 625,\n",
       " 'xb': 626,\n",
       " 'xc': 627,\n",
       " 'xd': 628,\n",
       " 'xe': 629,\n",
       " 'xf': 630,\n",
       " 'xg': 631,\n",
       " 'xh': 632,\n",
       " 'xi': 633,\n",
       " 'xj': 634,\n",
       " 'xk': 635,\n",
       " 'xl': 636,\n",
       " 'xm': 637,\n",
       " 'xn': 638,\n",
       " 'xo': 639,\n",
       " 'xp': 640,\n",
       " 'xq': 641,\n",
       " 'xr': 642,\n",
       " 'xs': 643,\n",
       " 'xt': 644,\n",
       " 'xu': 645,\n",
       " 'xv': 646,\n",
       " 'xw': 647,\n",
       " 'xx': 648,\n",
       " 'xy': 649,\n",
       " 'xz': 650,\n",
       " 'ya': 651,\n",
       " 'yb': 652,\n",
       " 'yc': 653,\n",
       " 'yd': 654,\n",
       " 'ye': 655,\n",
       " 'yf': 656,\n",
       " 'yg': 657,\n",
       " 'yh': 658,\n",
       " 'yi': 659,\n",
       " 'yj': 660,\n",
       " 'yk': 661,\n",
       " 'yl': 662,\n",
       " 'ym': 663,\n",
       " 'yn': 664,\n",
       " 'yo': 665,\n",
       " 'yp': 666,\n",
       " 'yq': 667,\n",
       " 'yr': 668,\n",
       " 'ys': 669,\n",
       " 'yt': 670,\n",
       " 'yu': 671,\n",
       " 'yv': 672,\n",
       " 'yw': 673,\n",
       " 'yx': 674,\n",
       " 'yy': 675,\n",
       " 'yz': 676,\n",
       " 'za': 677,\n",
       " 'zb': 678,\n",
       " 'zc': 679,\n",
       " 'zd': 680,\n",
       " 'ze': 681,\n",
       " 'zf': 682,\n",
       " 'zg': 683,\n",
       " 'zh': 684,\n",
       " 'zi': 685,\n",
       " 'zj': 686,\n",
       " 'zk': 687,\n",
       " 'zl': 688,\n",
       " 'zm': 689,\n",
       " 'zn': 690,\n",
       " 'zo': 691,\n",
       " 'zp': 692,\n",
       " 'zq': 693,\n",
       " 'zr': 694,\n",
       " 'zs': 695,\n",
       " 'zt': 696,\n",
       " 'zu': 697,\n",
       " 'zv': 698,\n",
       " 'zw': 699,\n",
       " 'zx': 700,\n",
       " 'zy': 701,\n",
       " 'zz': 702}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_to_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ss_to_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_to_i = {s: i+1 for i,s in enumerate(letters)}\n",
    "s_to_i['.'] = 0\n",
    "\n",
    "i_to_s = {i: s for s,i in s_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_to_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple counting model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. -> e\n",
      ".e -> m\n",
      "em -> m\n",
      "mm -> a\n",
      "ma -> .\n",
      ".. -> o\n",
      ".o -> l\n",
      "ol -> i\n",
      "li -> v\n",
      "iv -> i\n",
      "vi -> a\n",
      "ia -> .\n",
      ".. -> a\n",
      ".a -> v\n",
      "av -> a\n",
      "va -> .\n",
      "tensor(228146.) torch.Size([703, 27])\n"
     ]
    }
   ],
   "source": [
    "combinations = ['..'] + list(product('.', letters)) + list(product(letters, letters))\n",
    "\n",
    "ss_to_i = {c[0]+c[1]:i for i, c in enumerate(combinations)}\n",
    "\n",
    "for name in names[:3]:\n",
    "    name = '..' + name + '.'\n",
    "    for i,j,k in (zip(name, name[1:], name[2:])):\n",
    "        print(i+j, '->',k)\n",
    "\n",
    "N = torch.zeros((len(ss_to_i), len(s_to_i)))\n",
    "\n",
    "for name in names:\n",
    "    name = '..' + name + '.'\n",
    "    for i,j,k in (zip(name, name[1:], name[2:])):\n",
    "        row = ss_to_i[i+j]\n",
    "        col = s_to_i[k]\n",
    "        N[row, col] += 1\n",
    "print(N.sum(), N.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..mip\n",
      "..axx\n",
      "..mereyannyaar\n",
      "..knooraen\n",
      "..el\n",
      "..marviovania\n",
      "..odarimalaalexiaganilley\n",
      "..helahroni\n",
      "..haat\n",
      "..affiya\n",
      "..isemarrisleemikh\n",
      "..bech\n",
      "..amilleia\n",
      "..trutandenneppalycethon\n",
      "..jan\n",
      "..kryn\n",
      "..yusehanii\n",
      "..laymira\n",
      "..knoenoah\n",
      "..nowynni\n"
     ]
    }
   ],
   "source": [
    "# Smoothing\n",
    "# N = N+1\n",
    "\n",
    "# Use this model to generate some names:\n",
    "\n",
    "P = N / N.sum(1, keepdim=True)\n",
    "\n",
    "assert P[0].sum().item() - 1.0 < 1e-5\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    name = \"..\"\n",
    "    while True:\n",
    "        ss = name[-2:]\n",
    "        i = ss_to_i[ss]\n",
    "        sampled_i = int(torch.multinomial(P[i], 1, replacement=True, generator=g).item())\n",
    "        sampled_s = i_to_s[sampled_i]\n",
    "        if sampled_i == 0:\n",
    "            break\n",
    "        name += sampled_s\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll.item()=2.0619611740112305\n"
     ]
    }
   ],
   "source": [
    "# What's the negative loss likelihood of our model? Depends on how we do this, here we use \".name.\", which is wrong:\n",
    "\n",
    "sumlogprob = torch.tensor(0.0)\n",
    "count = 0\n",
    "for name in names:\n",
    "    name = '.' + name + '.'\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        prob = P[ss_to_i[ch1+ch2], s_to_i[ch3]]\n",
    "        logprob = torch.log(prob)\n",
    "        sumlogprob += logprob\n",
    "        count += 1\n",
    "nll = -sumlogprob / count\n",
    "print(f'{nll.item()=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll.item()=2.185652017593384\n"
     ]
    }
   ],
   "source": [
    "# What's the negative loss likelihood of our model? Depends on how we do this, here we use \"..name.\":\n",
    "\n",
    "sumlogprob = torch.tensor(0.0)\n",
    "count = 0\n",
    "for name in names:\n",
    "    name = '..' + name + '.'\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        prob = P[ss_to_i[ch1+ch2], s_to_i[ch3]]\n",
    "        logprob = torch.log(prob)\n",
    "        sumlogprob += logprob\n",
    "        count += 1\n",
    "nll = -sumlogprob / count\n",
    "print(f'{nll.item()=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of the 2D matrix that holds the counts, we can also use a 3D tensor of size 27x27x27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(228146.) torch.Size([27, 27, 27])\n"
     ]
    }
   ],
   "source": [
    "N = torch.zeros((len(s_to_i), len(s_to_i), len(s_to_i)))\n",
    "\n",
    "for name in names:\n",
    "    name = '..' + name + '.'\n",
    "    for i,j,k in (zip(name, name[1:], name[2:])):\n",
    "        ix1 = s_to_i[i]\n",
    "        ix2 = s_to_i[j]\n",
    "        ix3 = s_to_i[k]\n",
    "        N[ix1, ix2, ix3] += 1\n",
    "print(N.sum(), N.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..mip\n",
      "..axx\n",
      "..mereyannyaar\n",
      "..knooraen\n",
      "..el\n",
      "..marviovania\n",
      "..odarimalaalexiaganilley\n",
      "..helahroni\n",
      "..haat\n",
      "..affiya\n",
      "..isemarrisleemikh\n",
      "..bech\n",
      "..amilleia\n",
      "..trutandenneppalycethon\n",
      "..jan\n",
      "..kryn\n",
      "..yusehanii\n",
      "..laymira\n",
      "..knoenoah\n",
      "..nowynni\n"
     ]
    }
   ],
   "source": [
    "# Smoothing\n",
    "# N = N+100\n",
    "\n",
    "# Use this model to generate some names:\n",
    "\n",
    "P = N / N.sum(2, keepdim=True)\n",
    "\n",
    "assert P[0,0].sum().item() - 1.0 < 1e-5\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    name = \"..\"\n",
    "    while True:\n",
    "        ix1 = s_to_i[name[-2]]\n",
    "        ix2 = s_to_i[name[-1]]\n",
    "        sampled_i = int(torch.multinomial(P[ix1, ix2], 1, replacement=True, generator=g).item())\n",
    "        sampled_s = i_to_s[sampled_i]\n",
    "        if sampled_i == 0:\n",
    "            break\n",
    "        name += sampled_s\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll.item()=2.185652017593384\n"
     ]
    }
   ],
   "source": [
    "# What's the negative loss likelihood of our model?\n",
    "sumlogprob = torch.tensor(0.0)\n",
    "count = 0\n",
    "for name in names:\n",
    "    name = '..' + name + '.'\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        ix1 = s_to_i[ch1]\n",
    "        ix2 = s_to_i[ch2]\n",
    "        ix3 = s_to_i[ch3]\n",
    "        prob = P[ix1, ix2, ix3]\n",
    "        logprob = torch.log(prob)\n",
    "        sumlogprob += logprob\n",
    "        count += 1\n",
    "nll = -sumlogprob / count\n",
    "print(f'{nll.item()=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shows the exact same number as before, that's nice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to make the same model but then with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets:\n",
    "\n",
    "xs = []\n",
    "x1s = []\n",
    "x2s = []\n",
    "ys = []\n",
    "\n",
    "for name in names:\n",
    "    name = '..' + name + '.'\n",
    "    for x1, x2, y in zip(name, name[1:], name[2:]):\n",
    "        x1s.append(s_to_i[x1])\n",
    "        x2s.append(s_to_i[x2])\n",
    "        xs.append([s_to_i[x1], s_to_i[x2]])\n",
    "        ys.append(s_to_i[y])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "x1s = torch.tensor(x1s)\n",
    "x2s = torch.tensor(x2s)\n",
    "ys = torch.tensor(ys)\n",
    "num = len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 5]), tensor([ 0,  5, 13]))"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1s[:3], x2s[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0],\n",
       "        [ 0,  5],\n",
       "        [ 5, 13]])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.arange(3*3*3).reshape([3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.nn.functional.one_hot(torch.tensor(0), num_classes=3)\n",
    "x2 = torch.nn.functional.one_hot(torch.tensor(1), num_classes=3)\n",
    "xs = torch.nn.functional.one_hot(torch.tensor([0,1]), num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 10, 11])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 @ (x1 @ W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([3, 3, 3]))"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 9, 10, 11],\n",
       "        [18, 19, 20]])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x1 @ W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x2 @ (x1 @ W)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.nn.functional.one_hot(torch.tensor([0,0]), num_classes=3)\n",
    "x2 = torch.nn.functional.one_hot(torch.tensor([1,1]), num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (9x2 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [343]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (9x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "x2 @ (x1 @ W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([3, 3, 3]))"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x1 @ W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 0,  1,  2]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [ 9, 10, 11]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [18, 19, 20]]])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x1 @ W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (6x3 and 2x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [348]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (6x3 and 2x3)"
     ]
    }
   ],
   "source": [
    "x2.view(3,-1) @ (x1 @ W).view(3,3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.view(3,-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This really doesn't work, I don't know how we can use two one-hot encoded vectors and multiply that with W to get the row vector we are inte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 5]), tensor([ 0,  5, 13]))"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1s[:3], x2s[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.rand((27,27,27), dtype=torch.float, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.340085506439209\n",
      "3.1340880393981934\n",
      "3.0591394901275635\n",
      "2.8926663398742676\n",
      "2.837334156036377\n",
      "2.8620455265045166\n",
      "2.7483880519866943\n",
      "2.697908878326416\n",
      "2.733649730682373\n",
      "2.6369221210479736\n",
      "2.6320204734802246\n",
      "2.6898727416992188\n",
      "2.5979843139648438\n",
      "2.558075428009033\n",
      "2.5886223316192627\n",
      "2.524501323699951\n",
      "2.5174810886383057\n",
      "2.5000827312469482\n",
      "2.503399610519409\n",
      "2.508382797241211\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    logits = W[x1s, x2s, :]\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    nll = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(nll.item())\n",
    "    nll.backward()\n",
    "\n",
    "    W.data += -torch.tensor(200.0) * W.grad\n",
    "    W.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5790505409240723\n",
      "2.431211471557617\n",
      "2.401346445083618\n",
      "2.3636972904205322\n",
      "2.3635730743408203\n",
      "2.330418825149536\n",
      "2.337049722671509\n",
      "2.308791399002075\n",
      "2.3189427852630615\n",
      "2.2933919429779053\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    logits = W[x1s, x2s, :]\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    nll = -probs[torch.arange(num), ys].log().mean()\n",
    "    if i%10 == 0:\n",
    "        print(nll.item())\n",
    "    nll.backward()\n",
    "\n",
    "    W.data += -torch.tensor(150.0) * W.grad\n",
    "    W.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2422726154327393\n",
      "2.2320902347564697\n",
      "2.2253077030181885\n",
      "2.2203996181488037\n",
      "2.2166666984558105\n",
      "2.2137210369110107\n",
      "2.2113301753997803\n",
      "2.209345579147339\n",
      "2.2076680660247803\n",
      "2.206228733062744\n",
      "2.2049784660339355\n",
      "2.2038803100585938\n",
      "2.2029073238372803\n",
      "2.202038049697876\n",
      "2.2012557983398438\n",
      "2.200547933578491\n",
      "2.199903726577759\n",
      "2.199314594268799\n",
      "2.198773145675659\n",
      "2.1982738971710205\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    logits = W[x1s, x2s, :]\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    nll = -probs[torch.arange(num), ys].log().mean()\n",
    "    if i%100 == 0:\n",
    "        print(nll.item())\n",
    "    nll.backward()\n",
    "\n",
    "    W.data += -torch.tensor(100.0) * W.grad\n",
    "    W.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..mip.\n",
      "..axx.\n",
      "..mereyannyaar.\n",
      "..knooraen.\n",
      "..el.\n",
      "..marviovania.\n",
      "..odarimalaalexiaganilley.\n",
      "..helahroni.\n",
      "..haat.\n",
      "..affiya.\n",
      "..isemarrisleemikh.\n",
      "..bech.\n",
      "..amilleia.\n",
      "..trutandenneppalycethon.\n",
      "..jan.\n",
      "..kryn.\n",
      "..yusehanii.\n",
      "..laymira.\n",
      "..kni.\n",
      "..steferrysioratten.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the neural net model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    out = []\n",
    "    name = \"..\"\n",
    "    while True:\n",
    "        ix1 = s_to_i[name[-2]]\n",
    "        ix2 = s_to_i[name[-1]]\n",
    "\n",
    "        logits = W[ix1,ix2,:]\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum()\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        name += i_to_s[int(ix)]\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_threshold = int(len(names)*0.8)\n",
    "val_threshold = int(len(names)*0.9)\n",
    "\n",
    "train_names = names[:train_threshold]\n",
    "val_names = names[train_threshold:val_threshold]\n",
    "test_names = names[val_threshold:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alioune',\n",
       " 'alix',\n",
       " 'alois',\n",
       " 'alva',\n",
       " 'amirr',\n",
       " 'amrom',\n",
       " 'aniket',\n",
       " 'ansen',\n",
       " 'apolo',\n",
       " 'aqib']"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[train_threshold-5:train_threshold+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amirr', 'amrom')"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names[-1], val_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_names) + len(val_names) + len(test_names) == len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(182778.) torch.Size([703, 27])\n"
     ]
    }
   ],
   "source": [
    "N = torch.zeros((len(ss_to_i), len(s_to_i)))\n",
    "\n",
    "for name in train_names:\n",
    "    name = '..' + name + '.'\n",
    "    for i,j,k in (zip(name, name[1:], name[2:])):\n",
    "        row = ss_to_i[i+j]\n",
    "        col = s_to_i[k]\n",
    "        N[row, col] += 1\n",
    "print(N.sum(), N.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..mir\n",
      "..axx\n",
      "..merfynney\n",
      "..grahamir\n",
      "..ivaj\n",
      "..angerhyx\n",
      "..ron\n",
      "..na\n",
      "..ollah\n",
      "..daishaleilliencelbelyn\n",
      "..race\n",
      "..ta\n",
      "..ceevlah\n",
      "..heigh\n",
      "..roldjqjr\n",
      "..ai\n",
      "..ed\n",
      "..jilleia\n",
      "..trutcjlgmusqxdfzdevbwqplen\n",
      "..kryn\n"
     ]
    }
   ],
   "source": [
    "# Smoothing\n",
    "N = N+1\n",
    "\n",
    "# Use this model to generate some names:\n",
    "\n",
    "P = N / N.sum(1, keepdim=True)\n",
    "\n",
    "assert P[0].sum().item() == 1.0\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    name = \"..\"\n",
    "    while True:\n",
    "        ss = name[-2:]\n",
    "        i = ss_to_i[ss]\n",
    "        sampled_i = int(torch.multinomial(P[i], 1, replacement=True, generator=g).item())\n",
    "        sampled_s = i_to_s[sampled_i]\n",
    "        if sampled_i == 0:\n",
    "            break\n",
    "        name += sampled_s\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll.item()=2.053244113922119\n"
     ]
    }
   ],
   "source": [
    "# What's the negative loss likelihood of our model on train_names?\n",
    "sumlogprob = torch.tensor(0.0)\n",
    "count = 0\n",
    "for name in train_names:\n",
    "    name = '.' + name + '.'\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        prob = P[ss_to_i[ch1+ch2], s_to_i[ch3]]\n",
    "        logprob = torch.log(prob)\n",
    "        sumlogprob += logprob\n",
    "        count += 1\n",
    "nll = -sumlogprob / count\n",
    "print(f'{nll.item()=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll.item()=2.3264338970184326\n"
     ]
    }
   ],
   "source": [
    "# What's the negative loss likelihood of our model on val_names?\n",
    "sumlogprob = torch.tensor(0.0)\n",
    "count = 0\n",
    "for name in val_names:\n",
    "    name = '.' + name + '.'\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        prob = P[ss_to_i[ch1+ch2], s_to_i[ch3]]\n",
    "        logprob = torch.log(prob)\n",
    "        sumlogprob += logprob\n",
    "        count += 1\n",
    "nll = -sumlogprob / count\n",
    "print(f'{nll.item()=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll.item()=2.337472677230835\n"
     ]
    }
   ],
   "source": [
    "# What's the negative loss likelihood of our model on test_names?\n",
    "sumlogprob = torch.tensor(0.0)\n",
    "count = 0\n",
    "for name in test_names:\n",
    "    name = '.' + name + '.'\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        prob = P[ss_to_i[ch1+ch2], s_to_i[ch3]]\n",
    "        logprob = torch.log(prob)\n",
    "        sumlogprob += logprob\n",
    "        count += 1\n",
    "nll = -sumlogprob / count\n",
    "print(f'{nll.item()=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(182778.) torch.Size([703, 27])\n"
     ]
    }
   ],
   "source": [
    "N = torch.zeros((len(ss_to_i), len(s_to_i)))\n",
    "\n",
    "for name in train_names:\n",
    "    name = '..' + name + '.'\n",
    "    for i,j,k in (zip(name, name[1:], name[2:])):\n",
    "        row = ss_to_i[i+j]\n",
    "        col = s_to_i[k]\n",
    "        N[row, col] += 1\n",
    "print(N.sum(), N.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=0.0\n",
      "nll.item()=nan\n",
      "n=0.05\n",
      "nll.item()=2.3361752033233643\n",
      "n=0.1\n",
      "nll.item()=2.3272202014923096\n",
      "n=0.15000000000000002\n",
      "nll.item()=2.3231360912323\n",
      "n=0.2\n",
      "nll.item()=2.320887565612793\n",
      "n=0.25\n",
      "nll.item()=2.319657802581787\n",
      "n=0.30000000000000004\n",
      "nll.item()=2.319011926651001\n",
      "n=0.35000000000000003\n",
      "nll.item()=2.3187434673309326\n",
      "n=0.4\n",
      "nll.item()=2.31874942779541\n",
      "n=0.45\n",
      "nll.item()=2.318986415863037\n",
      "n=0.5\n",
      "nll.item()=2.3193023204803467\n",
      "n=0.55\n",
      "nll.item()=2.3197741508483887\n",
      "n=0.6000000000000001\n",
      "nll.item()=2.320326089859009\n",
      "n=0.65\n",
      "nll.item()=2.3209550380706787\n",
      "n=0.7000000000000001\n",
      "nll.item()=2.321634531021118\n",
      "n=0.75\n",
      "nll.item()=2.322371482849121\n",
      "n=0.8\n",
      "nll.item()=2.323124408721924\n",
      "n=0.8500000000000001\n",
      "nll.item()=2.3239188194274902\n",
      "n=0.9\n",
      "nll.item()=2.324730157852173\n",
      "n=0.9500000000000001\n",
      "nll.item()=2.3255748748779297\n",
      "n=1.0\n",
      "nll.item()=2.3264338970184326\n"
     ]
    }
   ],
   "source": [
    "for n in np.linspace(0,1,20+1):\n",
    "\n",
    "    print(f'{n=}')\n",
    "    \n",
    "    # Smoothing\n",
    "    NN = N+n\n",
    "\n",
    "    PP = NN / NN.sum(1, keepdim=True)\n",
    "\n",
    "    sumlogprob = torch.tensor(0.0)\n",
    "    count = 0\n",
    "    for name in val_names:\n",
    "        name = '.' + name + '.'\n",
    "        for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "            prob = PP[ss_to_i[ch1+ch2], s_to_i[ch3]]\n",
    "            logprob = torch.log(prob)\n",
    "            sumlogprob += logprob\n",
    "            count += 1\n",
    "    nll = -sumlogprob / count\n",
    "    print(f'{nll.item()=}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..mir\n",
      "..axx\n",
      "..mereyannya\n",
      "..salonaia\n",
      "..raad\n",
      "..marwinzephhara\n",
      "..ollah\n",
      "..daishaleilliencelbelyn\n",
      "..race\n",
      "..ta\n",
      "..ceevi\n",
      "..iselannamie\n",
      "..mell\n",
      "..ai\n",
      "..ed\n",
      "..jilleia\n",
      "..trutchelissitaey\n",
      "..crevilean\n",
      "..kryn\n",
      "..yuridanjine\n"
     ]
    }
   ],
   "source": [
    "# Smoothing\n",
    "NN = N+0.375\n",
    "\n",
    "# Use this model to generate some names:\n",
    "\n",
    "P = NN / NN.sum(1, keepdim=True)\n",
    "\n",
    "assert P[1].sum().item() - 1.0 < 1e-5\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    name = \"..\"\n",
    "    while True:\n",
    "        ss = name[-2:]\n",
    "        i = ss_to_i[ss]\n",
    "        sampled_i = int(torch.multinomial(P[i], 1, replacement=True, generator=g).item())\n",
    "        sampled_s = i_to_s[sampled_i]\n",
    "        if sampled_i == 0:\n",
    "            break\n",
    "        name += sampled_s\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=10.0\n",
      "nll=tensor(2.8295, grad_fn=<AddBackward0>)\n",
      "nll=tensor(2.8946, grad_fn=<AddBackward0>)\n",
      "r=1.0\n",
      "nll=tensor(2.3131, grad_fn=<AddBackward0>)\n",
      "nll=tensor(2.4403, grad_fn=<AddBackward0>)\n",
      "r=0.1\n",
      "nll=tensor(2.0189, grad_fn=<AddBackward0>)\n",
      "nll=tensor(2.1990, grad_fn=<AddBackward0>)\n",
      "r=0.01\n",
      "nll=tensor(1.9246, grad_fn=<AddBackward0>)\n",
      "nll=tensor(2.1490, grad_fn=<AddBackward0>)\n",
      "r=0.001\n",
      "nll=tensor(1.9056, grad_fn=<AddBackward0>)\n",
      "nll=tensor(2.1413, grad_fn=<AddBackward0>)\n",
      "r=0.0001\n",
      "nll=tensor(1.9034, grad_fn=<AddBackward0>)\n",
      "nll=tensor(2.1390, grad_fn=<AddBackward0>)\n",
      "r=1e-05\n",
      "nll=tensor(1.9058, grad_fn=<AddBackward0>)\n",
      "nll=tensor(2.1428, grad_fn=<AddBackward0>)\n",
      "r=1e-06\n",
      "nll=tensor(1.9058, grad_fn=<AddBackward0>)\n",
      "nll=tensor(2.1439, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x1s_train = x1s[:dev_threshold]\n",
    "x1s_val = x1s[dev_threshold:val_threshold]\n",
    "x1s_test = x1s[val_threshold:]\n",
    "\n",
    "x2s_train = x2s[:dev_threshold]\n",
    "x2s_val = x2s[dev_threshold:val_threshold]\n",
    "x2s_tes = x2s[val_threshold:]\n",
    "\n",
    "ys_train = ys[:dev_threshold]\n",
    "ys_val = ys[dev_threshold:val_threshold]\n",
    "ys_test = ys[val_threshold:]\n",
    "\n",
    "\n",
    "\n",
    "for r in [1e1, 1e0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]:\n",
    "\n",
    "    W = torch.rand((27,27,27), dtype=torch.float, requires_grad=True)\n",
    "    num = x1s_train.nelement()\n",
    "\n",
    "    for i in range(20):\n",
    "        logits = W[x1s_train, x2s_train, :]\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "        nll = -probs[torch.arange(num), ys_train].log().mean() + r * (W*W).mean()\n",
    "        nll.backward()\n",
    "\n",
    "        W.data += -torch.tensor(200.0) * W.grad\n",
    "        W.grad = None\n",
    "\n",
    "    for i in range(100):\n",
    "        logits = W[x1s_train, x2s_train, :]\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "        nll = -probs[torch.arange(num), ys_train].log().mean() + r * (W*W).mean()\n",
    "        nll.backward()\n",
    "\n",
    "        W.data += -torch.tensor(150.0) * W.grad\n",
    "        W.grad = None\n",
    "\n",
    "    for i in range(2000):\n",
    "        logits = W[x1s_train, x2s_train, :]\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "        nll = -probs[torch.arange(num), ys_train].log().mean() + r * (W*W).mean()\n",
    "        nll.backward()\n",
    "\n",
    "        W.data += -torch.tensor(100.0) * W.grad \n",
    "        W.grad = None\n",
    "    \n",
    "    print(f'{r=}')\n",
    "    print(f'{nll=}')\n",
    "    \n",
    "    \n",
    "    num = x1s_val.nelement()\n",
    "    logits = W[x1s_val, x2s_val, :]\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    nll = -probs[torch.arange(num), ys_val].log().mean() + r * (W*W).mean()\n",
    "    \n",
    "    print(f'{nll=}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we did that already above. Instead of using the one-hot vector we can just index into the matrix/tensor, since I couldn't make the one-hot vector approach work for the 3D tensor model\n",
    "\n",
    "We did this above when doing:\n",
    "\n",
    "```\n",
    "logits = W[x1s, x2s, :]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.tensor(np.arange(16))\n",
    "W = W.reshape([4,4])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enc = torch.nn.functional.one_hot(torch.tensor(3), num_classes=4)\n",
    "x_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 13, 14, 15])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 13, 14, 15])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (x_enc @ W == W[3]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, cross entropy is the same as softmax + nll (see also exercises from lesson 1)\n",
    "\n",
    "Cross entropy is probably doing some more optimizations, numerically more stable, less manual work so less probability of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c13b8769d29b9cf5cf47e8da9da13ab52310a6e33afb9474b0da20ac390dd33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
